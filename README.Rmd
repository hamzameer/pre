---
title: 'pre: an R package for deriving prediction rule ensembles'
output:
  md_document:
    variant: markdown_github
bibliography: README.bib
csl: inst/bib_style.csl
---


```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "inst/README-figures/README-",
  dpi = 124
)
```

**pre** is an **R** package for deriving prediction rule ensembles for binary, multinomial, (multivariate) continuous, count and survival responses. Input variables may be numeric, ordinal and categorical. An extensive description of the implementation and functionality is provided in [@Fokkema17]. The package largely implements the algorithm for deriving prediction rule ensembles as described in [@Friedman08], with several adjustments: 

1) The package is completely **R** based, allowing users better access to the results and more control over the parameters used for generating the prediction rule ensemble.
2) The unbiased tree induction algorithms of [@Hothorn06] is used for deriving prediction rules, by default. Alternatively, the (g)lmtree algorithm of [@Zeileis08] can be employed, or the classification and regression tree (CART) algorithm of [@Breiman84].
3) The package supports a wider range of response variable types.
5) The initial ensembles may be generated as in bagging, boosting and/or random forests.
6) Hinge functions of predictor variables may be included as baselearners, as in the multivariate adaptive regression splines method of [@Friedman91], using function `gpe()`.

Note that **pre** is under development, and much work still needs to be done. Below, a short introductory example is provided. [@Fokkema17] provides an extensive description of the fitting procedures implemented in function `pre()` and example analyses with more extensive explanations. 


## Example: Predicting ozone levels

To get a first impression of how function `pre()` works, we will fit a prediction rule ensemble to predict Ozone levels using the `airquality` dataset. We fit a prediction rule ensemble using function `pre()`:

```{r, results = FALSE}
library("pre")
airq <- airquality[complete.cases(airquality), ]
set.seed(42)
airq.ens <- pre(Ozone ~ ., data = airq)
```

Note that the random seed was set first, to allow for later replication of the results, as the fitting procedure depends on random sampling of training observations. 

We can print the resulting ensemble (alternatively, we could use the `print` method): 

```{r}
airq.ens
```

The cross-validated error printed here is calculated using the same data as was used for generating the rules and therefore may provide an overly optimistic estimate of future prediction error. To obtain a more realistic prediction error estimate, we will use function ```cvpre()``` later on. 

The table represents the rules and linear terms selected for the final ensemble, with the estimated coefficients. For rules, the `description` column provides the conditions. If all conditions of a rule apply to an observation, the predicted value of the response increases by the estimated coefficient, which is printed in the `coefficient` column. If linear terms were selected for the final ensemble (which is not the case here), the winsorizing points used to reduce the influence of outliers on the estimated coefficient would be printed in the `description` column. For linear terms, the estimated coefficient in `coefficient` reflects the increase in the predicted value of the response, for a unit increase in the predictor variable.

If we want to plot the rules in the ensemble as simple decision trees, we can use the `plot` method. Here, we request the nine most important baselearners are requested here through specification of the `nterms` argument. Through the `cex` argument, we specify the size of the node and path labels:

```{r treeplot, fig.show = "hide"}
plot(airq.ens, nterms = 9, cex = .5)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE, out.width = "600px"}
library("knitr")
include_graphics(sprintf("%streeplot-1.png", opts_current$get("fig.path")))
```

We can obtain the estimated coefficients for each of the baselearners using the `coef` method (only the first ten are printed here):

```{r}
coefs <- coef(airq.ens)
coefs[1:10,]
```

We can generate predictions for new observations using the `predict` method:

```{r}
predict(airq.ens, newdata = airq[1:4, ])
```

We can assess the expected prediction error of the prediction rule ensemble through cross validation (10-fold, by default) using the `cvpre()` function:

```{r}
set.seed(43)
airq.cv <- cvpre(airq.ens)
```

The results provide the mean squared error (MSE) and mean absolute error (MAE) with their respective standard errors. The cross-validated predictions, which can be used to compute alternative estimates of predictive accuracy, are saved in `airq.cv$cvpreds`. The folds to which observations were assigned are saved in `airq.cv$fold_indicators`.


## Tools for interpretation

Package **pre** provides several additional tools for interpretation of the final ensemble. These may be especially helpful for complex ensembles containing many rules and linear terms. 


### Importances

We can assess the relative importance of input variables as well as baselearners using the `importance()` function:

```{r importance, fig.show = "hide"}
imps <- importance(airq.ens, round = 4)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE, out.width = "400px"}
include_graphics(sprintf("%simportance-1.png", opts_current$get("fig.path")))
```

As we already observed in the printed ensemble, the plotted variable importances indicate that Temperature and Wind are most strongly associated with Ozone levels. Solar.R and Day are also associated with Ozone levels, but much less strongly. Variable Month is not plotted, which means it obtained an importance of zero, indicating that it is not associated with Ozone levels. We already observed this in the printed ensemble: Month was not selected as a linear term and did not appear in any of the selected rules. The variable and baselearner importances are saved in `imps$varimps` and `imps$baseimps`, respectively. 


### Explaining individual predictions

We can obtain explanations of the predictions for individual observations using function `explain()`:

```{r}
expl <- explain(airq.ens, newdata = airq[1:4, ], cex = .6)
```

The values of the rules and linear terms for each observation are saved in `expl$predictors`, their contributions in `expl$contribution` and the predicted values in `expl$predicted.value`.

We can assess correlations between the baselearners appearing in the ensemble using the `corplot()` function:

```{r corplot, fig.show='hide'}
corplot(airq.ens)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE, out.width = "500px"}
include_graphics(sprintf("%scorplot-1.png", opts_current$get("fig.path")))
```



### Partial dependence plots

We can obtain partial dependence plots to assess the effect of single predictor variables on the outcome using the `singleplot()` function:

```{r singleplot,  fig.show = "hide"}
singleplot(airq.ens, varname = "Temp")
```

```{r, echo=FALSE, warning=FALSE, message=FALSE, out.width = "400px"}
include_graphics(sprintf("%ssingleplot-1.png", opts_current$get("fig.path")))
```

We can obtain partial dependence plots to assess the effects of pairs of predictor variables on the outcome using the `pairplot()` function:

```{r pairplot, fig.show = "hide", warning=FALSE, message=FALSE}
pairplot(airq.ens, varnames = c("Temp", "Wind"))
```

```{r, echo=FALSE, warning=FALSE, message=FALSE, out.width = "400px"}
include_graphics(sprintf("%spairplot-1.png", opts_current$get("fig.path")))
```

Note that creating partial dependence plots is computationally intensive and computation time will increase fast with increasing numbers of observations and numbers of variables. `R` package `plotmo` created by Stephen @Milb18 provides more efficient functions for plotting partial dependence, which also support `pre` models. 

If the final ensemble does not contain a lot of terms, inspecting individual rules and linear terms through the `print` method may be (much) more informative than partial dependence plots. One of the main advantages of prediction rule ensembles is their interpretability: the predictive model contains only simple functions of the predictor variables (rules and linear terms), which are easy to grasp. Partial dependence plots are often much more useful for interpretation of complex models, like random forests for example.



### Assessing presence of interactions

We can assess the presence of interactions between the input variables using the `interact()` and `bsnullinteract()` funtions. Function `bsnullinteract()` computes null-interaction models (10, by default) based on bootstrap-sampled and permuted datasets. Function `interact()` computes interaction test statistics for each predictor variables appearing in the specified ensemble. If null-interaction models are provided through the `nullmods` argument, interaction test statistics will also be computed for the null-interaction model, providing a reference null distribution. 

Note that computing null interaction models and interaction test statistics is computationally very intensive.

```{r interact, eval=FALSE, fig.show = "hide"}
set.seed(44)
nullmods <- bsnullinteract(airq.ens)
int <- interact(airq.ens, nullmods = nullmods)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE, out.width = "350px"}
include_graphics(sprintf("%sinteract-1.png", opts_current$get("fig.path")))
```

The plotted variable interaction strengths indicate that Temperature and Wind may be involved in interactions, as their observed interaction strengths (darker grey) exceed the upper limit of the 90% confidence interval (CI) of interaction stengths in the null interaction models (lighter grey bar represents the median, error bars represent the 90% CIs). The plot indicates that Solar.R and Day are not involved in any interactions. Note that computation of null interaction models is computationally intensive. A more reliable result can be obtained by computing a larger number of boostrapped null interaction datasets, by setting the `nsamp` argument of function `bsnullinteract()` to a larger value (e.g., 100).


# Including hinge functions (multivariate adaptive regression splines)

More complex prediction ensembles can be obtained using the `gpe()` function. Abbreviation gpe stands for generalized prediction ensembles, which can also include hinge functions of the predictor variables as described in [@Friedman91], in addition to rules and/or linear terms. Addition of hinge functions may further improve predictive accuracy. See the following example:

```{r}
set.seed(42)
airq.gpe <- gpe(Ozone ~ ., data = airquality[complete.cases(airquality),], 
    base_learners = list(gpe_trees(), gpe_linear(), gpe_earth()))
airq.gpe
```



# References